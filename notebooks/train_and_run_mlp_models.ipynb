{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e071b84",
   "metadata": {},
   "source": [
    "# Train and Run the Underlying MLP Models in TELL Using Evolving Time Windows\n",
    "\n",
    "This notebook is the core tool for training and running the evolving sets of multilayer perceptron (MLP) models used in this experiment. It assumes that the base Total ELectricity Loads (TELL) model has already been installed and the datasets updated to include the most recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86db683a-70a2-4f89-a1d6-4c6d5f180272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start by importing the TELL package and information about your operating system:\n",
    "import os \n",
    "import tell\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e8454-24e1-44f2-bfb7-2f6499e11321",
   "metadata": {},
   "source": [
    "## Set the Directory Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa5a9e7b-a9a7-4fdd-957e-2bb5f063afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the top-level directory to store the trained MLP models:\n",
    "cleaned_ba_data_output_dir =  '/Users/burl878/Documents/Code/code_repos/burleyson-etal_2025_ldrd/data/cleaned_historical_data/'\n",
    "ba_to_process_input_dir =  '/Users/burl878/Documents/Code/code_repos/burleyson-etal_2025_ldrd/data/'\n",
    "weather_data_input_dir =  '/Users/burl878/Documents/Code/code_repos/burleyson-etal_2025_ldrd/data/historical_weather'\n",
    "model_output_directory = '/Users/burl878/Documents/Code/code_repos/burleyson-etal_2025_ldrd/data/trained_mlp_models/'\n",
    "prediction_output_directory = '/Users/burl878/Documents/Code/code_repos/burleyson-etal_2025_ldrd/data/mlp_projections/'\n",
    "composite_output_directory = '/Users/burl878/Documents/Code/code_repos/burleyson-etal_2025_ldrd/data/composite_projections/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359376d2-5a07-4868-892f-36db5931399d",
   "metadata": {},
   "source": [
    "## Set the List of Balancing Authorities to Analyze\n",
    "\n",
    "BAs used in this analysis are controlled by a master file `balancing_authorities_modeled.yml` stored in the `/data` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be839966-941b-4902-af8e-5de92b71b22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AZPS', 'BPAT', 'CISO', 'ERCO', 'FPL', 'ISNE', 'PJM', 'SWPP']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the yml file into a dictionary:\n",
    "with open((ba_to_process_input_dir + 'balancing_authority_modeled.yml'), 'r') as yml:\n",
    "     ba_list = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "     bas = [i for i in ba_list.keys()]\n",
    "\n",
    "# Return the list of BAs to process/plot:\n",
    "bas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a037554",
   "metadata": {},
   "source": [
    "## MLP Model Training\n",
    "\n",
    "The MLP models underpinning TELL use temporal variations in weather to project hourly demand. More information about this approach is in the MLP section of the `tell` [User Guide](https://immm-sfa.github.io/tell/user_guide.html). The default settings for the MLP model training steps are included in the `mlp_settings.yml` file included in the data folder of the `tell` repository. By default the MLP models are trained on data from 2016-2018 and evaluated using data from 2019. The time windows for training and evaluating the models can be modified by altering the `start_time`, `end_time`, and `split_datetime` parameters when calling the `tell.train` function. The workflow does not change the default MLP parameters (e.g., hidden layer sizes, maximum number of iterations, etc.) from TELL. Those could be modified in theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "439b742f-c5e9-487b-be65-ee3f25199ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module tell.mlp_train:\n",
      "\n",
      "train(region: str, data_dir: str, **kwargs)\n",
      "    Generate predictions for MLP model for a target region from an input CSV file.\n",
      "    \n",
      "    :param region:                      Indicating region / balancing authority we want to train and test on.\n",
      "                                        Must match with string in CSV files.\n",
      "    :type region:                       str\n",
      "    \n",
      "    :param data_dir:                    Full path to the directory that houses the input CSV files.\n",
      "    :type data_dir:                     str\n",
      "    \n",
      "    :param mlp_hidden_layer_sizes:      The ith element represents the number of neurons in the ith hidden layer.\n",
      "    :type mlp_hidden_layer_sizes:       Optional[int]\n",
      "    \n",
      "    :param mlp_max_iter:                Maximum number of iterations. The solver iterates until convergence\n",
      "                                        (determined by ‘tol’) or this number of iterations. For stochastic solvers\n",
      "                                        (‘sgd’, ‘adam’), note that this determines the number of epochs (how many\n",
      "                                        times each data point will be used), not the number of gradient steps.\n",
      "    :type mlp_max_iter:                 Optional[int]\n",
      "    \n",
      "    :param mlp_validation_fraction:     The proportion of training data to set aside as validation set for early\n",
      "                                        stopping. Must be between 0 and 1.\n",
      "    :type mlp_validation_fraction:      Optional[float]\n",
      "    \n",
      "    :param data_column_rename_dict:     Dictionary for the field names present in the input CSV file (keys) to what the\n",
      "                                        code expects them to be (values).\n",
      "    :type data_column_rename_dict:      Optional[dict[str]]\n",
      "    \n",
      "    :param expected_datetime_columns:   Expected names of the date time columns in the input CSV file.\n",
      "    :type expected_datetime_columns:    Optional[list[str]]\n",
      "    \n",
      "    :param hour_field_name:             Field name of the hour field in the input CSV file.\n",
      "    :type hour_field_name:              Optional[str]\n",
      "    \n",
      "    :param month_field_name:            Field name of the month field in the input CSV file.\n",
      "    :type month_field_name:             Optional[str]\n",
      "    \n",
      "    :param x_variables:                 Target variable list.\n",
      "    :type x_variables:                  Optional[list[str]]\n",
      "    \n",
      "    :param add_dayofweek_xvars:         True if the user wishes to add weekday and holiday targets to the x variables.\n",
      "    :type add_dayofweek_xvars:          Optional[bool]\n",
      "    \n",
      "    :param y_variables:                 Feature variable list.\n",
      "    :type y_variables:                  Optional[list[str]]\n",
      "    \n",
      "    :param day_list:                    List of day abbreviations and their order.\n",
      "    :type day_list:                     Optional[list[str]]\n",
      "    \n",
      "    :param start_time:                  Timestamp showing the datetime of for the run to start\n",
      "                                        (e.g., 2016-01-01 00:00:00).\n",
      "    :type start_time:                   Optional[str]\n",
      "    \n",
      "    :param end_time:                    Timestamp showing the datetime of for the run to end\n",
      "                                        (e.g., 2019-12-31 23:00:00).\n",
      "    :type end_time:                     Optional[str]\n",
      "    \n",
      "    :param split_datetime:              Timestamp showing the datetime to split the train and test data by\n",
      "                                        (e.g., 2018-12-31 23:00:00).\n",
      "    :type split_datetime:               Optional[str]\n",
      "    \n",
      "    :param nodata_value:                No data value in the input CSV file.\n",
      "    :type nodata_value:                 Optional[int]\n",
      "    \n",
      "    :param seed_value:                  Seed value to reproduce randomization.\n",
      "    :type seed_value:                   Optional[int]\n",
      "    \n",
      "    :param save_model:                  Choice to write ML models to a pickled file via joblib.\n",
      "    :type save_model:                   bool\n",
      "    \n",
      "    :param model_output_directory:      Full path to output directory where model file will be written.\n",
      "    :type model_output_directory:       Union[str, None]\n",
      "    \n",
      "    :param verbose:                     Choice to see logged outputs.\n",
      "    :type verbose:                      bool\n",
      "    \n",
      "    :return:                            [0] Predictions as a dataframe\n",
      "                                        [1] Summary statistics as a dataframe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For more information about the training of predictive models you can call the help function:\n",
    "help(tell.train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccddc4b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>predictions</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>14132.962047</td>\n",
       "      <td>22497.0</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>16730.206115</td>\n",
       "      <td>24297.0</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:00:00</td>\n",
       "      <td>17538.045667</td>\n",
       "      <td>26918.0</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:00:00</td>\n",
       "      <td>17851.649071</td>\n",
       "      <td>26849.0</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:00:00</td>\n",
       "      <td>17573.402353</td>\n",
       "      <td>25968.0</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-01 05:00:00</td>\n",
       "      <td>16992.306304</td>\n",
       "      <td>25044.0</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-01 06:00:00</td>\n",
       "      <td>16169.389993</td>\n",
       "      <td>24185.0</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-01 07:00:00</td>\n",
       "      <td>15322.279828</td>\n",
       "      <td>23305.0</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-01 08:00:00</td>\n",
       "      <td>14480.421379</td>\n",
       "      <td>22467.0</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-01 09:00:00</td>\n",
       "      <td>13801.735347</td>\n",
       "      <td>21810.0</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime   predictions  ground_truth region\n",
       "0 2020-01-01 00:00:00  14132.962047       22497.0   CISO\n",
       "1 2020-01-01 01:00:00  16730.206115       24297.0   CISO\n",
       "2 2020-01-01 02:00:00  17538.045667       26918.0   CISO\n",
       "3 2020-01-01 03:00:00  17851.649071       26849.0   CISO\n",
       "4 2020-01-01 04:00:00  17573.402353       25968.0   CISO\n",
       "5 2020-01-01 05:00:00  16992.306304       25044.0   CISO\n",
       "6 2020-01-01 06:00:00  16169.389993       24185.0   CISO\n",
       "7 2020-01-01 07:00:00  15322.279828       23305.0   CISO\n",
       "8 2020-01-01 08:00:00  14480.421379       22467.0   CISO\n",
       "9 2020-01-01 09:00:00  13801.735347       21810.0   CISO"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BA</th>\n",
       "      <th>RMS_ABS</th>\n",
       "      <th>RMS_NORM</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CISO</td>\n",
       "      <td>3502.927734</td>\n",
       "      <td>0.141187</td>\n",
       "      <td>0.132898</td>\n",
       "      <td>0.649472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BA      RMS_ABS  RMS_NORM      MAPE        R2\n",
       "0  CISO  3502.927734  0.141187  0.132898  0.649472"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the MLP training step for a single BA to get a feel of the functionality:\n",
    "prediction_df, validation_df = tell.train(region = 'CISO',\n",
    "                                          data_dir = cleaned_ba_data_output_dir,\n",
    "                                          start_time = '2018-01-01 00:00:00',\n",
    "                                          end_time = '2020-12-31 23:00:00',\n",
    "                                          split_datetime = '2019-12-31 23:00:00',\n",
    "                                          save_model = True,\n",
    "                                          model_output_directory = '/Users/burl878/Documents/Code/code_repos/burleyson-etal_2025_ldrd/data/trained_mlp_models/MLP1/')\n",
    "\n",
    "# View the head of the prediction dataframe that contains the time-series of projected load in the evaluation year:\n",
    "display(prediction_df.head(10))\n",
    "\n",
    "# View validation dataframe that contains error statistics for the trained model:\n",
    "validation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df28891-b1a0-430a-93b4-afa1e448027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training iteratively across all training windows and BAs:\n",
    "for model in ['MLP1', 'MLP2', 'MLP3', 'MLP4', 'MLP5', 'MLP6']:\n",
    "\n",
    "    # Set the training and evaluation periods for the specific model training window:\n",
    "    if model == 'MLP1':\n",
    "       start_time_model = '2016-01-01 00:00:00'\n",
    "       end_time_model = '2018-12-31 23:00:00'\n",
    "       split_datetime_model = '2017-12-31 23:00:00'\n",
    "    if model == 'MLP2':\n",
    "       start_time_model = '2017-01-01 00:00:00'\n",
    "       end_time_model = '2019-12-31 23:00:00'\n",
    "       split_datetime_model = '2018-12-31 23:00:00'\n",
    "    if model == 'MLP3':\n",
    "       start_time_model = '2018-01-01 00:00:00'\n",
    "       end_time_model = '2020-12-31 23:00:00'\n",
    "       split_datetime_model = '2019-12-31 23:00:00'\n",
    "    if model == 'MLP4':\n",
    "       start_time_model = '2019-01-01 00:00:00'\n",
    "       end_time_model = '2021-12-31 23:00:00'\n",
    "       split_datetime_model = '2020-12-31 23:00:00'\n",
    "    if model == 'MLP5':\n",
    "       start_time_model = '2020-01-01 00:00:00'\n",
    "       end_time_model = '2022-12-31 23:00:00'\n",
    "       split_datetime_model = '2021-12-31 23:00:00'\n",
    "    if model == 'MLP6':\n",
    "       start_time_model = '2021-01-01 00:00:00'\n",
    "       end_time_model = '2023-12-31 23:00:00'\n",
    "       split_datetime_model = '2022-12-31 23:00:00'\n",
    "\n",
    "    # Create the model output directory name:\n",
    "    output_directory = (model_output_directory + model + '/')\n",
    "  \n",
    "    # Check to see if the output directory exist and if not then create it:\n",
    "    if not os.path.exists(output_directory):\n",
    "       os.makedirs(output_directory)\n",
    "\n",
    "    # Loop over the eight BAs used in this LDRD analysis:\n",
    "    for ba in bas:\n",
    "        \n",
    "        # Run the MLP training for that BA and model training period:\n",
    "        prediction_df, validation_df = tell.train(region = ba,\n",
    "                                                  data_dir = cleaned_ba_data_output_dir,\n",
    "                                                  start_time = start_time_model,\n",
    "                                                  end_time = end_time_model,\n",
    "                                                  split_datetime = split_datetime_model,\n",
    "                                                  save_model = True,\n",
    "                                                  model_output_directory = output_directory)\n",
    "\n",
    "        # Print the model and BA combination to monitor the progress:\n",
    "        print('Model = ', model, ', BA = ', ba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4522eade-f01b-4683-9aea-e479761a43a8",
   "metadata": {},
   "source": [
    "## Project Forward Using the Trained MLP Models\n",
    "\n",
    "Use the TELL forward projection function to run the trained MLP models forward in time to project \"future\" loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2425f2-5bd9-4b04-be8e-a7cdef8bf5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more information about the how to use the models to project loads forward in time you can call the help function:\n",
    "help(tell.predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b182f-162b-461c-8a1b-d02dcd7ad37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the MLP forward projection step for a single BA to get a feel of the functionality:\n",
    "pdf = tell.predict(region = 'CISO',\n",
    "                   year = 2023,\n",
    "                   data_dir = weather_data_input_dir,\n",
    "                   datetime_field_name = 'Time_UTC',\n",
    "                   save_prediction = True,\n",
    "                   model_output_directory = '/Users/burl878/Documents/Code/code_repos/burleyson-etal_2025_ldrd/data/trained_mlp_models/MLP1',\n",
    "                   prediction_output_directory = '/Users/burl878/Documents/Code/code_repos/burleyson-etal_2025_ldrd/data/mlp_projections/MLP1')\n",
    "\n",
    "pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9884d35-ba87-4aa0-8739-e066f5237daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the forward projection step iteratively across all training windows and BAs:\n",
    "for model in ['MLP1', 'MLP2', 'MLP3', 'MLP4', 'MLP5', 'MLP6']:\n",
    "\n",
    "    # Set the first forward year for each model:\n",
    "    if model == 'MLP1':\n",
    "       first_forward_year = 2018\n",
    "    if model == 'MLP2':\n",
    "       first_forward_year = 2019\n",
    "    if model == 'MLP3':\n",
    "       first_forward_year = 2020\n",
    "    if model == 'MLP4':\n",
    "       first_forward_year = 2021\n",
    "    if model == 'MLP5':\n",
    "       first_forward_year = 2022\n",
    "    if model == 'MLP6':\n",
    "       first_forward_year = 2023\n",
    "\n",
    "    # Create the model output directory name:\n",
    "    output_directory = (model_output_directory + model + '/')\n",
    "  \n",
    "    # Check to see if the output directory exist and if not then create it:\n",
    "    if not os.path.exists(output_directory):\n",
    "       os.makedirs(output_directory)\n",
    "\n",
    "    # Loop over the eight BAs used in this LDRD analysis:\n",
    "    for ba in bas:\n",
    "\n",
    "        # Loop over the years from the first forward year for that model through 2023:\n",
    "        for year in range(first_forward_year,2024,1):\n",
    "        \n",
    "            # Run the MLP forward projection for that BA, model, and year:\n",
    "            pdf = tell.predict(region = ba,\n",
    "                               year = year,\n",
    "                               data_dir = weather_data_input_dir,\n",
    "                               datetime_field_name = 'Time_UTC',\n",
    "                               save_prediction = True,\n",
    "                               model_output_directory = (model_output_directory + model + '/'),\n",
    "                               prediction_output_directory = (prediction_output_directory + model + '/'))\n",
    "\n",
    "            # Print the model and BA combination to monitor the progress:\n",
    "            print('Model = ', model, ', BA = ', ba, ', Year = ', str(year))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0249f-b898-495b-81d7-7bbc4d33c147",
   "metadata": {},
   "source": [
    "## Compile the Historical and Forward Projection Datasets into a Single .csv File for Each BA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabb7ae-23a8-44ce-9c46-f148ab52f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the eight BAs used in this LDRD analysis:\n",
    "for ba in bas:\n",
    "\n",
    "    # Load in the compiled historical data from the TELL repository:\n",
    "    base_df = pd.read_csv((cleaned_ba_data_output_dir + ba + '_cleaned_historical_data.csv'), index_col=None, header=0)\n",
    "\n",
    "    # Convert the time columns into one datetime variable:\n",
    "    base_df['Time_UTC'] = pd.to_datetime(base_df[['Year', 'Month', 'Day', 'Hour']])\n",
    "    \n",
    "    # Convert the temperature from Kelvin to Fahrenheit:\n",
    "    base_df['T2'] = ((1.8 * (base_df['T2'] - 273)) + 32).round(2)\n",
    "\n",
    "    # Round the population to the nearest whole integer and demand to the nearest tenth:\n",
    "    base_df['Cleaned_Demand_MWh'] = base_df['Cleaned_Demand_MWh'].round(1)\n",
    "    base_df['Total_Population'] = base_df['Total_Population'].round(0).astype(int)\n",
    "    \n",
    "    # Rename the demand and population columns to something simpler:\n",
    "    base_df.rename(columns={'Cleaned_Demand_MWh': 'Demand_MWh', 'Total_Population': 'Population'}, inplace=True)\n",
    "\n",
    "    # Add in the BA code for better tracking:\n",
    "    base_df['BA'] = ba\n",
    "    \n",
    "    # Only keep the columns that are needed:\n",
    "    base_df = base_df[['BA', 'Time_UTC', 'T2', 'Q2', 'SWDOWN', 'GLW', 'WSPD', 'Population', 'Demand_MWh']].copy()\n",
    "\n",
    "    # Loop over the different models and merge in their demand projections iteratively:\n",
    "    for model in ['MLP1', 'MLP2', 'MLP3', 'MLP4', 'MLP5', 'MLP6']:\n",
    "\n",
    "        # Set the first forward year for each model:\n",
    "        if model == 'MLP1':\n",
    "           first_forward_year = 2018\n",
    "        if model == 'MLP2':\n",
    "           first_forward_year = 2019\n",
    "        if model == 'MLP3':\n",
    "           first_forward_year = 2020\n",
    "        if model == 'MLP4':\n",
    "           first_forward_year = 2021\n",
    "        if model == 'MLP5':\n",
    "           first_forward_year = 2022\n",
    "        if model == 'MLP6':\n",
    "           first_forward_year = 2023\n",
    "\n",
    "        # Loop over the years from the first forward year for that model through 2023:\n",
    "        for year in range(first_forward_year,2024,1):\n",
    "\n",
    "            # Load in the MLP projection for that model and year combination:\n",
    "            proj_df = pd.read_csv((prediction_output_directory + model + '/' + str(year) + '/' + ba  + '_' + str(year) + '_mlp_output.csv'), index_col=None, header=0)\n",
    "\n",
    "            # Only keep the columns that are needed:\n",
    "            proj_df = proj_df[['Time_UTC', 'Load']].copy()\n",
    "\n",
    "            # Round the demand to the nearest tenth:\n",
    "            proj_df['Load'] = proj_df['Load'].round(1)\n",
    "    \n",
    "            # Rename the load variable to reflect the model used to generate the projection:\n",
    "            proj_df.rename(columns={'Load': (model + '_MWh')}, inplace=True)\n",
    "\n",
    "            # Convert the time to a datetime variable:\n",
    "            proj_df['Time_UTC'] = pd.to_datetime(proj_df['Time_UTC'])\n",
    "\n",
    "            # Aggregate the output into a new dataframe:\n",
    "            if year == first_forward_year:\n",
    "               aggregate_proj_df = proj_df\n",
    "            else:\n",
    "               aggregate_proj_df = pd.concat([aggregate_proj_df, proj_df])\n",
    "            \n",
    "        # Merge the base_df and proj_df dataframes using common UTC times:\n",
    "        base_df = base_df.merge(aggregate_proj_df, on=['Time_UTC'], how='left')\n",
    "\n",
    "    # Copy the data into an output dataframe:\n",
    "    output_df = base_df.copy()\n",
    "\n",
    "    # Replace NaN values with -999:\n",
    "    output_df = output_df.fillna(-999)\n",
    "\n",
    "    # Set the output file name:\n",
    "    csv_output_filename = (composite_output_directory + ba + '_Composite_Data.csv')\n",
    "\n",
    "    # Write out the dataframe to a .csv file:\n",
    "    output_df.to_csv(csv_output_filename, sep=',', index=False)\n",
    "\n",
    "    # Clean up the variables and move to the next BA in the loop:\n",
    "    del base_df, model, first_forward_year, year, proj_df, aggregate_proj_df, output_df, csv_output_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e90bee-f0ff-47fc-a664-3611ffe00785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tell",
   "language": "python",
   "name": "tell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
